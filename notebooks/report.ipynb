{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977457a5",
   "metadata": {},
   "source": [
    "# DRAFT Analysis Report\n",
    "\n",
    "## Context\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Identify a list of endpoints representing where the data is stored/catalogued, \n",
    "- Harvest the endpoints as possible,\n",
    "- Analyse those data for fair-interoperability work done up till now,\n",
    "- Report the results & translate into recommendations \n",
    "\n",
    "## Analysis methods\n",
    "- for each of the given endpoints, assessment of the steps needed to find and access the data & other aspects that reduce FAIRness of data:\n",
    "    - manual assessment of pitfalls regarding data interoperability and re-usability \n",
    "    - is data easily findable & accessible?\n",
    "    - what is the granularity of the available data? \n",
    "        - metadata on the dataset\n",
    "        - dataset as a file\n",
    "        - datapoints \n",
    "    - are semantics clear and unambiguous of the data offered by the endpoint?\n",
    "    - are data easily integratable with other sources?\n",
    "\n",
    "These analyses can be consulted in the jupyter notebooks.  \n",
    "\n",
    "## Results & Discussion\n",
    "\n",
    "The analyses indicated an overall good level of FAIRness at a basic level. Given the endpoints and documentation, data is generally findable, accessible.  \n",
    "\n",
    "There is a certain level of standardization present in the way data are made available:\n",
    "    - at level of the service:  some services were developed following a standard (e.g. SensorThings API, swagger APIs, ERDDAP).\n",
    "    - at level of the data offered by the service: e.g. use of same column headers across similar kinds of services.\n",
    "However, there is still room for improvement as semantics are not always clear.  \n",
    "\n",
    "Furthermore, finding, accessing and using data though the offered services makes the assumption that domain specific knowledge is present on how to handle data. In case one doesn't know how to handle e.g. netCDF files, accessing and using the data becomes more difficult and can only be accomplished after a learning curve.\n",
    "\n",
    "-------\n",
    "Data granularity goes to data file level ...\n",
    "    \n",
    "\n",
    "though on interoperability & reusability level mistakes are easily made \n",
    "- getting the data and using it in analyses requires domain specific knowledge \n",
    "- combining data requires lots of additional steps\n",
    "- semantics of the data is not always clear, \n",
    "    - mistakes during analysis more likely\n",
    "    - & which imposes another threshold to combining data \n",
    "-------\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "Overall interoperability within services is good at basic level. Data is generally findable, accessible & useable.\n",
    "\n",
    "- If you want data to be used at wider scale, they it should be more self-descriptive (because then you cannot assume domain knowledge to be present & without being more self descriptive analysis mistakes are very likely to occur)\n",
    "\n",
    "though there is still room for improvement\n",
    "to increase interoperability, recommendations at 2 levels:\n",
    "\n",
    "1. Description of services\n",
    "    - describe the offered services (~ the endpoints analysed) via LD ---> todo: provide an example!!\n",
    "    - a quick fix, \n",
    "    - to improve finding your way around available services & data, & more quickly determine which service one can use keeping in mind inhouse knowledge (e.g. having someone who can work with json API, S3 buckets, ...)  \n",
    "\n",
    "2. Common data model\n",
    "    - develop a data model, with various stakeholders from the community, to better describe the offered data \n",
    "      agree on the common entities that are described (instruments, events, observations, ...), and with which properties they're described \n",
    "    - tis would allow to more easily integrate data from different sources\n",
    "    - and make this data available as LOD in the future\n",
    "        - one can get inspiration from the ARGO & ICOS data models in their respective SPARQL endpoints\n",
    "        - other common ontologies:\n",
    "            - prov\n",
    "            - dct\n",
    "            - ssn\n",
    "            - qube --> very suitable for the description of NetCDF files\n",
    "            - dcat\n",
    "            - ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9213dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
