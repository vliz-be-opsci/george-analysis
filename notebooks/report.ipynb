{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977457a5",
   "metadata": {},
   "source": [
    "# DRAFT Analysis Report\n",
    "\n",
    "## Context\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Identify a list of endpoints representing where the data is stored/catalogued, \n",
    "- Harvest the endpoints as possible,\n",
    "- Analyse those data for fair-interoperability work done up till now,\n",
    "- Report the results & translate into recommendations \n",
    "\n",
    "## Analysis methods & results\n",
    "- for each of the given endpoints, a manual assessment of the FAIRness & interoperability of the offered data. \n",
    "- the FAIRness & interoperability assessment consisted of checking:\n",
    "    - how easily can data be found and accessed?\n",
    "    - what is the data granularity? i.e. to what level is data available? \n",
    "    - is data semantically unambiguous and interoperable?\n",
    "    - how easily is data integratable?\n",
    "\n",
    "These analyses can be consulted in the jupyter notebooks.  \n",
    "\n",
    "## Findings \n",
    "\n",
    "General findings:\n",
    "\n",
    "Overall good level of FAIRness at a basic level. \n",
    "1. Given the endpoints, documentation and domain specific knowledge, data is generally findable, accessible and usable.  \n",
    "\n",
    "2. Interoperability/standardization  \n",
    "Certain level of standardization present offered data:\n",
    "    - at level of the service/endpoint:  some services were developed following a standard (e.g. SensorThings API, swagger APIs, ERDDAP).\n",
    "    - at level of the data offered by the service: e.g. \n",
    "        - use of same column headers across similar kinds of services. \n",
    "        - in some cases use of standard terms such as orcID, urls, ... \n",
    "\n",
    "However, there is still room for improvement:  (maybe move this to recommendations)\n",
    "    - unambiguous semantics: use of codes, more & more consistent use of externally defined terms \n",
    "    - alignment of data model structure across endpoints, currently internal data structure is exposed via the endpoints (good practice seen in ICOS SPARQL endpoint --> offer a dcat description --> domain of ocean observation would benefit from a common data model structure to expose data given that data is similar in kind/type/nature across endpoints)     \n",
    "\n",
    "\n",
    "3. Domain specific knowledge is required to be able to access and use the data  \n",
    "    - on the level of type of endpoint and data format:\n",
    "    in order to access the data via the given endpoint, one must know how to navigate that type of endpoint, be it a JSON API, ERDDAP server, SPARQL endpoint  \n",
    "    additionally, one msut also know how to work with the file format in which the data is offered (with this project, most data is offered as netCDF, JSON, ...)\n",
    "\n",
    "    - on the level fo the data model:\n",
    "    in order to use the data, one must know what the data(points) represents and this requires knowledge on how the data is modelled  \n",
    "\n",
    "--> consequences:\n",
    "    - domain specific knowledge either is available inhouse or needs to be obtained trough learning curve. \n",
    "    - makes data less interopreable & (re-)usable\n",
    "    - ...\n",
    "\n",
    "3. Granularity of data offered by endpoint varies:\n",
    "    - sometimes to file level, other times to observation level\n",
    "    - in most cases, can go to observations/measurments levels with additional steps (e.g. retrieval of data from within files) \n",
    "    - ...\n",
    "\n",
    "4. Data integration is possible but hindered by the required domain specific knowledge & unambiguous semantics\n",
    "    - also makes mistakes more likely when combining data \n",
    "    - ...\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "Overall FAIRness & interoperability of services is good at basic level. Data is findable, accessible & useable.\n",
    "\n",
    "However, if data is to be used at wider scale, one cannot assume domain specific knowledge to be present, and hence data should become more self-descriptive.  \n",
    "(because then you cannot assume domain knowledge to be present & without being more self descriptive analysis mistakes are very likely to occur)\n",
    "\n",
    "Formulated recommendations for improved self-descriptivness of data at 2 levels:\n",
    "\n",
    "1. Description of available services\n",
    "    - describe the offered services (~ the endpoints analysed) via LD ---> todo: provide an example!!\n",
    "        - more a quick fix, \n",
    "        - to improve finding your way around available services & data, & more quickly determine which service one can use keeping in mind inhouse knowledge (e.g. having someone who can work with json API, S3 buckets, ...)  \n",
    "    - more use of external standard terms, \n",
    "        - e.g. use of OrcID, ROR ID, urls for observed parameters, ...\n",
    "        - but would be best if chosen terms are aligned acros stakeholders, (see also second recommendation)\n",
    "\n",
    "\n",
    "2. Common data model\n",
    "    - develop a data model, with various stakeholders from the community, to better describe the offered data \n",
    "      agree on the common entities that are described (instruments, events, observations, ...), and with which properties they're described \n",
    "    - tis would allow to more easily integrate data from different sources\n",
    "    - and make this data available as LOD in the future\n",
    "        - one can get inspiration from the ARGO & ICOS data models in their respective SPARQL endpoints\n",
    "        - other common ontologies:\n",
    "            - prov\n",
    "            - dct\n",
    "            - ssn\n",
    "            - qube --> very suitable for the description of NetCDF files\n",
    "            - dcat\n",
    "            - ...\n",
    "\n",
    "3. overall complexity of systems hindered quantitive analysis\n",
    "\n",
    "4. We'll do an analysis of degree of alignment between data properties --> TODO: make lists of properties for each endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9213dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
